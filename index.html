<html>
    <head>
        <style>
            body {
                padding: 5em;
                max-width: 1200px;
                margin: auto;
                text-align: left;
                font-family: 'Verdana',Roboto,arial,sans-serif;
            }
            p {
                font-weight: 100;
                font-size: 1.1em;
                text-align: left;
            }
            h1 {
                display: block;
                font-size: 2em;
                margin-block-start: 0.67em;
                margin-block-end: 0.67em;
                margin-inline-start: 0px;
                margin-inline-end: 0px;
                font-weight: 400;
            }
            h2 {
                font-weight: 400;
            }
            .special {
                align: center;
                max-width: 50%;
                width: 50%;
                height: auto;
                margin-left: auto;
                margin-right: auto;
            }

            .special-large {
                align: center;
                max-width: 80%;
                width: 80%;
                height: auto;
                margin-left: auto;
                margin-right: auto;
            }

            figure {
                width:50%;
            }
            figure figcaption {
              border: 1px dotted blue;
              text-align: center;
            }
            tr {
                display: table-row;
                vertical-align: inherit;
                border-color: inherit;
            }
            img {
                width: 100%;
                height: 100%;
                object-fit: cover;
            }
            /* .author-block {
                display: inline-block;
            } */

          .publication-authors {
              font-family: 'Google Sans', sans-serif;
          }
          .publication-links {
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 3em;
            margin-top: 1em
            /* width: 50%; */
            /* gap: 10px; */
          }
          .link-block {
            height: 40px;
            width: 40px;
            margin: 20px;
            /* flex-basis: 100%; */
            /* width: 0; */


          }
          .authors {
            display: flex;
            align-items: center;
            align-self: center;
            align-content: center;
            justify-content: center;
            /* width: 50%; */
            /* gap: 10px; */
            margin: 1.5em;
            margin-bottom: 0.5em;
            font-size: 20;
            margin-left: 0;
            margin-right: 0;
          }
          /* .author-block {
            height: 10px;
            width: 150px;
            margin: 0px;
            align: center;
          } */
          .authors div {
            flex: 0 0 auto;
          }
          .first{
            width: 150px;
          }
          .second {
            width: 148px;
          }
          .third {
            width: 144px;
          }
          .fourth {
            width: 162px;
          }
          .fifth {
            width: 170px;
          }
          .sixth {
            width: 170px;
          }
          .author-block1 {
            height: 10px;
            width: 150px;
            margin: 0px;
          }
          img { width: 100%; height: auto; }
          a{
            color: #3273dc;
    cursor: pointer;
    text-decoration: none;
    text-decoration-line: none;
    text-decoration-thickness: initial;
    text-decoration-style: initial;

          }
        </style>
        <title>SAQ: Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning
        </title>
    </head>
    <body>
        <center>
            <h1>SAQ: Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning
            </h1>
        </center>

        <div class="authors">
          <div class="first">
              <a style="text-decoration:none" href="https://people.eecs.berkeley.edu/~jianlanluo/">Jianlan Luo</a></div>
          <div class="second">
              <a style="text-decoration:none" href="https://pd-perry.github.io">Perry Dong</a></div>
          <div class="third">
              <a style="text-decoration:none" href="">Jeffrey Wu</a>
          </div>
          <div class="fourth">
              <a style="text-decoration:none" href="https://aviralkumar2907.github.io/">Aviral Kumar</a>
          </div>
          <div class="fifth"z>
              <a style="text-decoration:none" href="https://young-geng.xyz/">Xinyang Geng</a>
          </div>
          <div class="sixth">
              <a style="text-decoration:none" href="https://cs.berkeley.edu/~svlevine">Sergey Levine</a>
          </div>
      </div>


          <div class="publication-links">

              <!-- Code Link. -->
              <div class="link-block">
                  <a href="https://github.com/jianlanluo/SAQ">
                      <img src="github.png" ></img>
                      <center><span>Code</span></center>
                  </a>
              </div>


              <!-- BibTex -->
              <span class="link-block">
                  <a href="https://arxiv.org/pdf/2310.11731.pdf">
                      <img src="paper.png"></img>
                      <center><span>Paper</span></center>
                  </a>
              </span>

          </div>

        <center>
            <h2>Abstract
            </h2>
        </center>

        <center>
            <p>The offline reinforcement learning (RL) paradigm provides a general
recipe to convert static behavior datasets of experience into policies that can
perform better than the behavior policy that collected the data. While policy
constraints, conservatism, and other methods for mitigating distributional shifts
w.r.t. behavior datasets have made offline reinforcement learning more effective,
the continuous action setting often necessitates various approximations for applying
these techniques. Many of these challenges are greatly alleviated in discrete action
settings, where offline RL constraints and regularizers can often be computed more
precisely or even exactly. In this paper, we propose an adaptive scheme for action
quantization. We use a VQ-VAE to learn state-conditioned action quantization,
avoiding the exponential blowup that comes with na√Øve discretization of the action
space. We show that several state-of-the-art offline RL methods such as IQL,
CQL, and BRAC improve in performance on benchmarks when combined with
our proposed discretization scheme. We further validate our approach on a set of
challenging long-horizon complex robotic manipulation tasks in the robomimic
environment where our discretized offline RL algorithms are able to improve upon
their continuous counterparts by 2-3x in performance. Our project page is at
saqrl.github.io.
            </p>
            <br>
        </center>

        <center>
            <h2>Method
            </h2>
        </center>

        <div wdith="50%">
        <img src="saq_train.gif">
        <img src="saq_inference.gif">
      </div>



    </body>
</html>
